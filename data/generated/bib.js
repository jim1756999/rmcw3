const generatedBibEntries = {
    "RN41": {
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.",
        "author": "Kaiming, He and Xiangyu, Zhang and Shaoqing, Ren and Jian, Sun",
        "keywords": "type: analysis, time: static, paradigm: other, evaluation: benchmark, application: image, category: Classification",
        "pages": "770-778",
        "title": "Deep Residual Learning for Image Recognition",
        "type": "misc Generic",
        "url": "http://image-net.org/challenges/LSVRC/2015/",
        "year": "2016"
    },
    "RN42": {
        "abstract": "Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic images and giving rise to a multitude of applications. However, the effective integration of T2I models into fundamental image classification tasks remains an open question. A prevalent strategy to bolster image classification performance is through augmenting the training set with synthetic images generated by T2I models. In this study, we scrutinize the shortcomings of both current generative and conventional data augmentation techniques. Our analysis reveals that these methods struggle to produce images that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts. To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix (https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by performing image translations between classes. Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifications for domain-specific datasets.",
        "author": "Zhicai, Wang and Longhui, Wei and Tan, Wang and Heyu, Chen and Yanbin, Hao and Xiang, Wang and Xiangnan, He and Qi, Tian",
        "doi": "10.1109/CVPR52733.2024.01630",
        "issn": "9798350353006 10636919",
        "keywords": "type: technique, time: static, paradigm: other, evaluation: benchmark, application: document, category: Classification",
        "title": "Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model",
        "type": "article @article{RN42, Journal Article",
        "url": "https://arxiv.org/pdf/2403.19600",
        "year": "2024"
    },
    "RN43": {
        "abstract": "We present the latest generation of MobileNets: MobileNetV4 (MNv4). They feature universally-efficient architecture designs for mobile devices. We introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted...",
        "author": "Danfeng, Qin and Chas, Leichner and Manolis, Delakis and Marco, Fornoni and Shixin, Luo and Fan, Yang and Weijun, Wang and Colby, Banbury and Chengxi, Ye and Berkin, Akin and Vaibhav, Aggarwal and Tenghui, Zhu and Daniele, Moro and Andrew, Howard",
        "doi": "10.1007/978-3-031-73661-2_5",
        "issn": "978-3-031-73661-2 1611-3349",
        "journal": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
        "keywords": "type: analysis, time: static, paradigm: other, evaluation: none, application: mobile, category: Classification",
        "pages": "78-96 , publisher",
        "title": "MobileNetV4: Universal Models for\u00a0the\u00a0Mobile Ecosystem",
        "type": "article @article{RN43, Journal Article",
        "url": "https://link.springer.com/chapter/10.1007/978-3-031-73661-2_5",
        "volume": "15098 LNCS",
        "year": "2025"
    },
    "RN44": {
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
        "author": "Joseph, Redmon and Santosh, Divvala and Ross, Girshick and Ali, Farhadi",
        "keywords": "type: technique, time: static, paradigm: node-link, evaluation: benchmark, application: image, category: Object Detection",
        "pages": "779-788",
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "type": "misc Generic",
        "url": "http://pjreddie.com/yolo/",
        "year": "2016"
    },
    "RN45": {
        "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9\u00d7 faster than R-CNN, is 213\u00d7 faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3\u00d7 faster, tests 10\u00d7 faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.",
        "author": "Ross, Girshick",
        "keywords": "type: technique, time: static, paradigm: node-link, evaluation: benchmark, application: general, category: Object Detection",
        "pages": "1440-1448",
        "title": "Fast R-CNN",
        "type": "misc Generic",
        "url": "https://github.com/rbgirshick/",
        "year": "2015"
    },
    "RN46": {
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build fully convolutional networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
        "author": "Jonathan, Long and Evan, Shelhamer and Trevor, Darrell",
        "keywords": "type: technique, time: static, paradigm: node-link, evaluation: none, application: image, category: Segmentation",
        "pages": "3431-3440",
        "title": "Fully Convolutional Networks for Semantic Segmentation",
        "type": "misc Generic",
        "year": "2015"
    },
    "RN47": {
        "abstract": "Premise: Quantitative plant traits play a crucial role in biological research. However, traditional methods for measuring plant morphology are time consuming and have limited scalability. We present LeafMachine2, a suite of modular machine learning and computer vision tools that can automatically extract a base set of leaf traits from digital plant data sets. Methods: LeafMachine2 was trained on 494,766 manually prepared annotations from 5648 herbarium images obtained from 288 institutions and representing 2663 species; it employs a set of plant component detection and segmentation algorithms to isolate individual leaves, petioles, fruits, flowers, wood samples, buds, and roots. Our landmarking network automatically identifies and measures nine pseudo-landmarks that occur on most broadleaf taxa. Text labels and barcodes are automatically identified by an archival component detector and are prepared for optical character recognition methods or natural language processing algorithms. Results: LeafMachine2 can extract trait data from at least 245 angiosperm families and calculate pixel-to-metric conversion factors for 26 commonly used ruler types. Discussion: LeafMachine2 is a highly efficient tool for generating large quantities of plant trait data, even from occluded or overlapping leaves, field images, and non-archival data sets. Our project, along with similar initiatives, has made significant progress in removing the bottleneck in plant trait data acquisition from herbarium specimens and shifted the focus toward the crucial task of data revision and quality control.",
        "author": "William, N. Weaver and Stephen, A. Smith",
        "doi": "10.1002/aps3.11548",
        "issn": "21680450 , issue",
        "journal": "Applications in Plant Sciences",
        "keywords": "digital extended specimen,digital specimen voucher,herbarium,machine learning,morphometrics,neural networks,phenology,traits; type: analysis, time: static, paradigm: node-link, evaluation: none, application: document, category: Biodiversity",
        "title": "From leaves to labels: Building modular machine learning networks for rapid herbarium specimen analysis with LeafMachine2",
        "type": "article Journal Article",
        "volume": "11",
        "year": "2023"
    },
    "RN48": {
        "abstract": "Nonrandom collecting practices may bias conclusions drawn from analyses of herbarium records. Recent efforts to fully digitize and mobilize regional floras online offer a timely opportunity to assess commonalities and differences in herbarium sampling biases. We determined spatial, temporal, trait, phylogenetic, and collector biases in c. 5 million herbarium records, representing three of the most complete digitized floras of the world: Australia (AU), South Africa (SA), and New England, USA (NE). We identified numerous shared and unique biases among these regions. Shared biases included specimens collected close to roads and herbaria; specimens collected more frequently during biological spring and summer; specimens of threatened species collected less frequently; and specimens of close relatives collected in similar numbers. Regional differences included overrepresentation of graminoids in SA and AU and of annuals in AU; and peak collection during the 1910s in NE, 1980s in SA, and 1990s in AU. Finally, in all regions, a disproportionately large percentage of specimens were collected by very few individuals. We hypothesize that these mega-collectors, with their associated preferences and idiosyncrasies, shaped patterns of collection bias via \u2018founder effects\u2019. Studies using herbarium collections should account for sampling biases, and future collecting efforts should avoid compounding these biases to the extent possible.",
        "author": "Barnabas, H. Daru and Daniel, S. Park and Richard, B. Primack and Charles, G. Willis and David, S. Barrington and Timothy, J. S. Whitfeld and Tristram, G. Seidler and Patrick, W. Sweeney and David, R. Foster and Aaron, M. Ellison and Charles, C. Davis",
        "doi": "10.1111/NPH.14855,",
        "issn": "14698137 , issue",
        "journal": "New Phytologist",
        "keywords": "collector bias,geographic bias,herbarium,regional flora,sampling bias,temporal bias,trait bias; type: analysis, time: animation, paradigm: map, evaluation: none, application: biology, category: Biodiversity",
        "pages": "939-955 , pmid",
        "title": "Widespread sampling biases in herbaria revealed from large-scale digitization",
        "type": "article Journal Article",
        "url": "https://pubmed.ncbi.nlm.nih.gov/29083043/",
        "volume": "217",
        "year": "2018"
    },
    "RN49": {
        "abstract": "Herbarium specimens provide verifiable and citable evidence of the occurrence of particular plants at particular points in space and time, and are vital resources for assessing extinction risk in t...",
        "author": "Eimear Nic, Lughadha and Barnaby, E. Walker and C\u00e1tia, Canteiro and Helen, Chadburn and Aaron, P. Davis and Serene, Hargreaves and Eve, J. Lucas and Andr\u00e9, Schuiteman and Emma, Williams and Steven, P. Bachman and David, Baines and Amy, Barker and Andrew, P. Budden and Julia, Carretero and James, J. Clarkson and Alexandra, Roberts and Malin, C. Rivers",
        "doi": "10.1098/RSTB.2017.0402",
        "issn": "14712970 , issue",
        "journal": "Philosophical Transactions of the Royal Society B",
        "keywords": "IUCN Red List,conservation assessment,digitization,extent of occurrence,machine learning,natural history collections,plant science Keywords: natural history collections; type: analysis, time: static, paradigm: other, evaluation: none, application: biology, category: Biodiversity",
        "title": "The use and misuse of herbarium specimens in evaluating plant extinction risks",
        "type": "article Journal Article",
        "url": "/doi/pdf/10.1098/rstb.2017.0402",
        "volume": "374",
        "year": "2019"
    },
    "RN50": {
        "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5\u00d7 smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code is available at: github.com/NVlabs/SegFormer.",
        "author": "Enze, Xie and Wenhai, Wang and Zhiding, Yu and Anima, Anandkumar and Jose, M. Alvarez and Ping, Luo",
        "journal": "Advances in Neural Information Processing Systems",
        "keywords": "type: technique, time: static, paradigm: other, evaluation: benchmark, application: general, category: Segmentation",
        "pages": "12077-12090",
        "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
        "type": "article @article{RN50, Journal Article",
        "volume": "34",
        "year": "2021"
    }
};